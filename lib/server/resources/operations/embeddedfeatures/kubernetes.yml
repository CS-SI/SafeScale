# Copyright 2018-2021, CS Systemes d'Information, http://www.c-s.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
feature:
    suitableFor:
        host: no
        cluster: all

    requirements:
        features:
            - docker
            - certificateauthority

        # "clusterSizing" not honored currently
        clusterSizing:
            boh:
                small:
                    masters: "count >= 1, cpu >= 2"
                    nodes:   "count >= 1, cpu >= 2"
                normal:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 1, cpu >= 2"
                large:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 3, cpu >= 2"
            k8s:
                small:
                    masters: "cpu >= 2"
                    nodes:   "cpu >= 2"
                normal:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 3, cpu >= 2"
                large:
                    masters: "count >= 5, cpu >= 2"
                    nodes:   "count >= 8, cpu >= 2"

    parameters:
        - AllowPodsOnMasters=false
        - AllowPodsOnGateways=false
        - CNI=flannel
        #- CNI=calico
        - CalicoNetworking=vxlan
        - ClusterControlplaneEndpointIP
        - Dashboard=true
        - EnableAudit=false
        - ExternalLoadBalancerHostname=
        - Hardening=true
        - KubeVersion=1.20.9
        # - CalicoVersion=v3.19.1
        - FlannelVersion=v0.13.0
        - OIDCClientName=default
        - OIDCGroupName=user_groups
        - OIDCRealm=
        - OIDCUserName=username

    install:
        bash:
            check:
                pace: nodes,kube,ready
                steps:
                    nodes:
                        targets:
                            gateways: all
                            nodes: all
                        run: |
                            if [ -f /etc/kubernetes/.joined ]; then
                                pidof kubelet &>/dev/null || sfFail 192 "kubelet not running"
                                sfExit
                            fi
                            sfFail 193 "node didn't join (no .joined file)"

                    kube:
                        targets:
                            masters: one
                        run: |
                            if [ -f /etc/kubernetes/.joined ]; then
                                [ $(sfKubectl get nodes -A | wc -l) -gt 1 ] || sfFail 194 "no nodes joined yet"
                                sfExit
                            fi
                            sfFail 195 "master didn't join (no .joined file)"
                    ready:
                        targets:
                            masters: one
                        run: |
                            if [ -f /etc/kubernetes/.joined ]; then
                                [ $(sfKubectl get nodes -A | grep -v VERSION | grep -v Ready | wc -l) -eq 0 ] || sfFail 196 "not all nodes ready"
                                sfExit
                            fi
                            sfFail 197 "master didn't join (no .joined file)"

            add:
                pace: ca-certs,sysconf,syshardening,common-tools,ca,halb,cp1-init,cni,cpx-init,join-gws,join-nodes,fw,supplemental,final
                steps:
                    ca-certs:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            # Fixes Let's Encrypt root certificate expiration error with wget occurring 09/30/21
                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfWaitForApt
                                    sfRetry sfApt install -y ca-certificates || sfFail 192
                                    ;;
                                redhat|rhel|fedora|centos)
                                    sfRetry yum install -y ca-certificates || sfFail 193
                                    ;;
                                *)
                                    echo "Unmanaged Linux distribution '$LINUX_KIND'"
                                    sfFail 194
                                    ;;
                            esac
                            sfExit

                    sysconf:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            # Enabling kernel modules required by Kubernetes
                            for i in ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh br_netfilter; do
                                echo $i >>/etc/modules-load.d/kubernetes.conf
                                modprobe $i 2>/dev/null || sfFail 201 "Error enabling kernel module $i, error code $?"
                            done
    
                            op=-1
                            for i in nf_conntrack_ipv4 nf_conntrack; do
                                echo $i >>/etc/modules
                                modprobe $i 2>/dev/null && op=0 || true
                            done
                            if [ $op -eq -1 ]; then
                                sfFail 192 "Error enabling kernel module nf_conntrack and nf_conntrack_ipv4"
                            fi

                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfWaitForApt
                                    sfRetry sfApt install -y ebtables socat || sfFail 193 "Error installing etables and socat"
                                    ;;
                                redhat|rhel|fedora|centos)
                                    sfRetry yum install -y ebtables socat || sfFail 194 "Error installing etables and socat"

                                    cat >/etc/sysctl.d/kubernetes.conf <<-'EOF'
                            net.bridge.bridge-nf-call-ip6tables = 1
                            net.bridge.bridge-nf-call-iptables = 1
                            net.bridge.bridge-nf-call-arptables = 1
                            EOF

                                    # Set SELinux in permissive mode (effectively disabling it)
                                    if [[ -n $(command -v getenforce) ]]; then
                                        act=0
                                        getenforce | grep "Disabled" || act=1
                                        if [ $act -eq 1 ]; then
                                            if [[ -n $(command -v setenforce) ]]; then
                                                setenforce 0 || sfFail 201 "Error setting selinux in Permissive mode"
                                                sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
                                            fi
                                        fi
                                    fi
                                    ;;
                                *)
                                    echo "Unmanaged Linux distribution '$LINUX_KIND'"
                                    sfFail 195 "Unmanaged Linux distribution '$LINUX_KIND'"
                                    ;;
                            esac

                            kernel_version=$(cat /proc/version | awk '{ print $3 }' | cut -d. -f1)

                            echo "net.ipv4.ip_forward=1" >>/etc/sysctl.d/99-sysctl.conf
                            sysctl --system

                            # Disable swap if enabled
                            op=-1
                            SWAPS=$(grep "swap[[:space:]]*sw[[:space:]]*" /etc/fstab | column -t | cut -d' ' -f1) && op=$? || true
                            if [ $op -eq 0 ]; then
                                cp /etc/fstab /etc/fstab.before_swap_disable
                                for s in $SWAPS; do
                                    swapoff $s &>/dev/null
                                    grep -v "$s" /etc/fstab >>/etc/fstab.new && mv /etc/fstab.new /etc/fstab
                                done
                            fi
                            sfExit

                    syshardening:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            {{ if eq .Hardening "true" }}
                                {{ if eq .EnableAudit "true" }}
                                case $(sfGetFact "linux_kind") in
                                    debian|ubuntu)
                                        sfApt update
                                        sfRetry sfApt install -y auditd quota quotatool || sfFail 196 "Error installing auditd quota quotatool"
                                        ;;

                                    centos|fedora|redhat|rhel)
                                        sfRetry yum -y install audit audit-libs quota || sfFail 197 "Error installing audit audit-libs quota"
                                        ;;

                                    *)
                                        echo "unsupported linux distribution '$(sfGetFact "linux_kind")'"
                                        sfFail 198 "unsupported linux distribution '$(sfGetFact "linux_kind")'"
                                        ;;
                                esac

                                ## Configures ulimit - not compatible with a host on which a user graphical system is used - like remote desktop...
                                # cat >> /etc/security/limits.conf <<EOF
                                # *      soft      nproc      100
                                # *      hard      nproc      200
                                # *      soft      nofile      1024
                                # *      hard      nofile      2048
                                # EOF

                                ## Configures audit rules
                                cat >/etc/audit/rules.d/docker.rules <<EOF
                                -w /usr/bin/docker -p awx -k docker_binary
                                -w /var/lib/docker -p aw -k docker_data
                                -w /etc/docker -p aw -k docker_configuration
                                -w /etc/docker/daemon.json -p aw -k docker_daemon
                                -w /etc/default/docker -p aw -k docker_default
                                -w /usr/bin/containerd -p awx -k docker_containerd
                                -w /usr/bin/runc -p awx -k docker_runc
                                EOF
                                sfService restart auditd

                                {{ end }}
                            # Disable docker swarm
                            cat /etc/docker/daemon.json | jq '. + { "swarm-default-advertise-addr": "" }' >/etc/docker/daemon.json.new && \
                            mv /etc/docker/daemon.json.new /etc/docker/daemon.json && \
                            sfService restart docker || sfFail 199 "failed to disable docker swarm"
                            {{ end }}
                            sfExit

                    common-tools:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            case $(sfGetFact "linux_kind") in
                                debian|ubuntu)
                                    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
                                    cat >/etc/apt/sources.list.d/kubernetes.list <<-EOF
                            deb https://apt.kubernetes.io/ kubernetes-xenial main
                            EOF
                            {{ if .KubeVersion }}
                                    sfApt update && sfApt install -y kubelet={{.KubeVersion}}-00 kubeadm={{.KubeVersion}}-00 kubectl={{.KubeVersion}}-00 || sfFail 199 "Error installing kubelet kubadm kubectl"
                            {{ else }}
                                    sfApt update && sfApt install -y kubelet kubeadm kubectl || sfFail 200 "Error installing kubelet kubeadm kubectl"
                            {{ end }}
                                    apt-mark hold kubelet kubeadm kubectl
                                    ;;

                                centos|fedora|redhat|rhel)
                                    cat >/etc/yum.repos.d/kubernetes.repo <<-EOF
                            [kubernetes]
                            name=Kubernetes
                            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
                            enabled=1
                            gpgcheck=1
                            repo_gpgcheck=1
                            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
                            exclude=kube*
                            EOF

                            {{ if .KubeVersion }}
                                    yum -y install kubelet-{{.KubeVersion}} kubeadm-{{.KubeVersion}} kubectl-{{.KubeVersion}} --disableexcludes=kubernetes || sfFail 199 "Error installing kubelet kubeadm kubectl"
                            {{ else }}
                                    yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes || sfFail 201 "Error installing kubelet kubeadm kubectl"
                            {{ end }}
                                    ;;

                                *) echo "unsupported linux distribution '$(sfGetFact "linux_kind")'"
                                   sfFail 202 "unsupported linux distribution '$(sfGetFact "linux_kind")'"
                            esac
                            sfService enable kubelet && sfService start kubelet

                            sfExit

                    halb:
                        targets:
                            masters: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            {{ if .ClusterControlplaneUsesVIP }}
                            case $(sfGetFact "linux_kind") in
                                ubuntu|debian)
                                    sfApt update && sfApt -y install keepalived haproxy || sfFail 203 "Error installing keepalived haproxy"
                                    ;;
                                redhat|rhel|centos|fedora)
                                    yum install -q -y keepalived haproxy || sfFail 204 "Error installing keepalived haproxy"
                                    ;;
                                *)
                                    sfFail 205 "Unsupported Linux distribution '$(sfGetFact "linux_kind")'!"
                                    ;;
                            esac

                            NETMASK=$(echo {{ .CIDR }} | cut -d/ -f2)
                            IDX={{ .Hostname }}
                            IDX=${IDX##*-}
                            cat >/etc/keepalived/keepalived.conf <<-EOF
                            vrrp_instance vrrp_group_controlplane {
                                state BACKUP
                                interface $(sfInterfaceWithIP {{ .HostIP }})
                                virtual_router_id 1
                                priority ${IDX}
                                nopreempt
                                advert_int 2
                                authentication {
                                    auth_type PASS
                                    auth_pass "{{ .ClusterAdminPassword }}"
                                }
                                # Unicast specific option, this is the IP of the interface keepalived listens on
                                unicast_src_ip {{ .HostIP }}
                                # Unicast specific option, this is the IP of the peer instance
                                unicast_peer {
                                  {{- range .ClusterMasterIPs }}
                                    {{ if ne . $.HostIP }}{{ . }}{{ end }}
                                  {{- end }}
                                }
                                virtual_ipaddress {
                                    {{ .ClusterControlplaneEndpointIP }}/${NETMASK}
                                }
                            }
                            EOF
                            chown -R root:root /etc/keepalived
                            chmod -R u+rw,g+r-w,o-rwx /etc/keepalived
                            chmod ug+x /etc/keepalived

                            cat >/etc/sysctl.d/22-keepalived.conf <<-EOF
                            net.ipv4.ip_forward=1
                            net.ipv4.ip_nonlocal_bind=1
                            EOF
                            sysctl --system

                            if [ "$(sfGetFact "use_systemd")" = "1" ]; then
                                # Use systemd to ensure keepalived is restarted if network is restarted
                                # (otherwise, keepalived is in undetermined state)
                                mkdir -p /etc/systemd/system/keepalived.service.d
                                if [ "$(sfGetFact "redhat_like")" = "1" ]; then
                                    cat >/etc/systemd/system/keepalived.service.d/override.conf <<EOF
                            [Unit]
                            Requires=network.service
                            PartOf=network.service
                            EOF
                                else
                                    cat >/etc/systemd/system/keepalived.service.d/override.conf <<EOF
                            [Unit]
                            Requires=systemd-networkd.service
                            PartOf=systemd-networkd.service
                            EOF
                                fi
                                systemctl daemon-reload
                            fi

                            sfService enable keepalived
                            sfService restart keepalived || sfFail 206 "Error restarting keepalived"

                            ## Configure haproxy as TCP load-balancer for kubernetes Control Plane
                            cat >/etc/haproxy/haproxy.cfg <<-EOF
                            global
                                log /dev/log    local0
                                log /dev/log    local1 notice
                                chroot /var/lib/haproxy
                                stats socket /var/local/haproxy/admin.sock mode 660 level admin
                                stats timeout 30s
                                user haproxy
                                group haproxy
                                pidfile     /var/run/haproxy.pid
                                maxconn     4000
                                daemon

                                # Default SSL material locations
                                ca-base /etc/ssl/certs
                                crt-base /etc/ssl/private

                                # Default ciphers to use on SSL-enabled listening sockets.
                                # For more information, see ciphers(1SSL). This list is from:
                                #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
                                # An alternative list with additional directives can be obtained from
                                #  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy
                                ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
                                ssl-default-bind-options no-sslv3

                            defaults
                                mode tcp
                                log global
                                option httplog
                                option dontlognull
                                option http-server-close
                                option redispatch
                                retries 3
                                timeout http-request    10s
                                timeout queue           1m
                                timeout connect         10s
                                timeout client          1m
                                timeout server          1m
                                timeout http-keep-alive 10s
                                timeout check           10s
                                maxconn                 3000

                            listen kube_controlplane
                                bind {{ .ClusterControlplaneEndpointIP }}:6443
                                mode tcp
                                option tcplog
                                timeout client 10m
                                timeout server 10m
                                balance leastconn
                                {{- range $i, $ip := .ClusterMasterIPs }}
                                server cp{{ $i }} {{.}}:6444 check fall 3 rise 2
                                {{- end }}
                            EOF
                            mkdir -p /var/local/haproxy
                            sfService enable haproxy
                            sfService restart haproxy || sfFail 207 "Error restarting haproxy"
                            {{ end }}
                            sfExit

                    ca:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            mkdir -p /etc/kubernetes/pki
                            if [ -f ${SF_ETCDIR}/pki/ca/certs/rootca.cert.pem ]; then
                                [ -f /etc/kubernetes/pki/ca.crt ] && mv /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.crt.notused
                                [ -f /etc/kubernetes/pki/ca.key ] && mv /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key.notused
                                cp ${SF_ETCDIR}/pki/ca/certs/rootca.cert.pem /etc/kubernetes/pki/ca.crt
                                cp ${SF_ETCDIR}/pki/ca/private/rootca.key.pem /etc/kubernetes/pki/ca.key
                            fi
                            chown -R root:{{ .ClusterAdminUsername }} /etc/kubernetes/pki
                            sfExit

                    cp1-init:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            mkdir -p /etc/kubernetes/kubeadm

                            {{ if .KubeVersion }}
                            KUBE_VERSION=v{{ .KubeVersion }}
                            {{ else }}
                            KUBE_VERSION=stable
                            {{ end }}

                            {{ if .ClusterControlplaneUsesVIP }}
                            LOCAL_PORT=6444
                            CP_ENDPOINT_IP={{ .ClusterControlplaneEndpointIP}}
                            {{ else }}
                            LOCAL_PORT=6443
                            CP_ENDPOINT_IP={{ .HostIP }}
                            {{ end }}

                            DNSDOMAIN=$(hostname -d)
                            [ -z "${DNSDOMAIN}" ] && DNSDOMAIN=cluster.local

                            cat >/etc/kubernetes/kubeadm/kubeadm-config.yaml <<-EOF
                            ---
                            apiVersion: kubeadm.k8s.io/v1beta2
                            kind: ClusterConfiguration
                            controlPlaneEndpoint: "${CP_ENDPOINT_IP}:6443"
                            kubernetesVersion: ${KUBE_VERSION}
                            etcd:
                                local:
                                    dataDir: /var/lib/etcd
                            networking:
                                dnsDomain: ${DNSDOMAIN}
                                podSubnet: 10.244.0.0/16
                                serviceSubnet: 10.96.0.0/12
                            {{ if eq .Hardening "true" }}
                            apiServer:
                                timeoutForControlPlane: 4m0s
                                extraArgs:
                                    #? API server is restarted because of health check fail
                                    #? Modern K8S deploys preferes RBAC admission associate to PodSecurityPolicy to authorization control
                                    # anonymous-auth: "false"
                                    ##? EventRateLimit error running kubeadm init, alpha version
                                    ##? DenyEscalatingExec, deprecated
                                    enable-admission-plugins: AlwaysPullImages,DefaultStorageClass,NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurityPolicy
                                    ## disable-admission-plugins: DynamicAuditing
                                    ##? error running kubeadm init
                                    # RBAC is used to grant permission to client, restrict access to anonymous users
                                    authorization-mode: RBAC,Node
                                    #!allow-privileged: "false"
                                    ##? kube-proxy cannot be run
                                    ##? RBAC admission provide fine grained control
                                    audit-log-path: /var/log/k8s_audit.log
                                    audit-log-maxage: "30"
                                    audit-log-maxbackup: "10"
                                    audit-log-maxsize: "100"
                                    #? crash during init phase, to be investigated ...
                                    ## encryption-provider-config: aescbc
                                    profiling: "false"
                            controllerManager:
                                extraArgs:
                                    profiling: "false"
                                    use-service-account-credentials: "true"
                                    feature-gates: RotateKubeletServerCertificate=true
                                    cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
                                    cluster-signing-key-file: /etc/kubernetes/pki/ca.key
                            ---
                            apiVersion: kubeadm.k8s.io/v1beta2
                            kind: InitConfiguration
                            localAPIEndpoint:
                                bindPort: ${LOCAL_PORT}
                            {{ end }}
                            certificatesDir: /etc/kubernetes/pki
                            clusterName: kubernetes
                            dns:
                                type: CoreDNS
                            imageRepository: k8s.gcr.io
                            {{ if .OIDCRealm }}
                                {{ if .ExternalLoadBalancerHostname }}
                            oidc-issuer-url: "https://{{ .ExternalLoadBalancerHostname }}/realms/{{ .OIDCRealm }}"
                                {{ else }}
                            oidc-issuer-url: "https://{{ .EndpointIP }}/realms/{{ .OIDCRealm }}"
                            # oidc-ca-file: "/opt/safescale/etc/pki/ca/certs/rootca.cert.pem"
                                {{ end }}
                            oidc-client-id: "{{ .OIDCClientName }}"
                            oidc-username-claim: "{{ .OIDCUserName }}"
                            oidc-groups-claim: "{{ .OIDCGroupName }}"
                            {{ end }}
                            ---
                            apiVersion: kubeproxy.config.k8s.io/v1alpha1
                            kind: KubeProxyConfiguration
                            metricsBindAddress: 0.0.0.0:10249
                            EOF

                            sfRetryEx {{ or .reserved_DockerImagePullTimeout "10m" }} {{ or .reserved_DefaultDelay 10 }} kubeadm config images pull || sfFail 208 "Kubeadm error pulling images"
                            sudo kubeadm init --config=/etc/kubernetes/kubeadm/kubeadm-config.yaml || sfFail 209 "Kubeadm error init config"
                            cp_join_cmd=$(kubeadm token create --ttl 10m --print-join-command) || sfFail 210 "Kubeadm error creating tokens"
                            cert_key=$(kubeadm init phase upload-certs --upload-certs | tail -n 1) || sfFail 211 "Kubeadm error uploading certs"

                            cat >${SF_TMPDIR}/init_cluster_admin_kube.sh <<-'EOF'
                            mkdir -p ~{{.ClusterAdminUsername}}/.kube
                            cp -f /etc/kubernetes/admin.conf ~{{.ClusterAdminUsername}}/.kube/config
                            chown -R {{.ClusterAdminUsername}}:{{.ClusterAdminUsername}} ~{{.ClusterAdminUsername}}/.kube && \
                            chmod -R go-rwx ~{{.ClusterAdminUsername}}/.kube

                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/ca.crt
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/server.key
                            sudo chmod g+r /etc/kubernetes/pki/etcd/server.key
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/server.crt
                            EOF

                            # execute init_cluster_admin_kube.sh on the current master
                            bash -x ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 212 "Kubeadm error initializing cluster"
                            # Starting from here, any kubectl command must be changed to sfKubectl

                            # Push control-plane join command to all the other masters
                            cat >${SF_TMPDIR}/cp_join_cmd.sh <<-EOF
                            kubeadm reset --force
                            $cp_join_cmd --control-plane --certificate-key $cert_key --apiserver-bind-port $LOCAL_PORT
                            EOF
                            sfDropzonePush ${SF_TMPDIR}/cp_join_cmd.sh || sfFail 213 "Error uploading cp_join_cmd.sh"
                            sfDropzonePush ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 214 "Error uploading init_cluster_admin_kube.sh"
                            for ip in {{range .ClusterMasterIPs}}{{.}} {{end}}; do
                                [ "$ip" = "{{.HostIP}}" ] && continue
                                sfDropzoneSync $ip || sfFail 215 "Error in sync"
                            done
                            rm ${SF_TMPDIR}/cp_join_cmd.sh ${SF_TMPDIR}/init_cluster_admin_kube.sh
                            sfDropzoneClean

                            {{ if eq .Hardening "true" }}
                            # define and apply Pod Security Polices (PSP)
                            cat >/etc/kubernetes/kubeadm/psp-default.yaml <<-'EOF'
                            apiVersion: policy/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: default
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
                                seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
                            spec:
                              privileged: false
                              allowPrivilegeEscalation: false
                              allowedCapabilities: []  # default set of capabilities are implicitly allowed
                              volumes:
                              - 'configMap'
                              - 'emptyDir'
                              - 'projected'
                              - 'secret'
                              - 'downwardAPI'
                              - 'persistentVolumeClaim'
                              hostNetwork: false
                              hostIPC: false
                              hostPID: false
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            rules:
                            - apiGroups: ['policy']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['default']
                            - apiGroups: ['extensions']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['default']
                            ---
                            kind: ClusterRoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            roleRef:
                              kind: ClusterRole
                              name: default-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                            - kind: Group
                              name: system:authenticated
                              apiGroup: rbac.authorization.k8s.io
                            EOF

                            cat >/etc/kubernetes/kubeadm/psp-privileged.yaml <<-'EOF'
                            apiVersion: policy/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: privileged
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
                            spec:
                              privileged: true
                              allowPrivilegeEscalation: true
                              allowedCapabilities: ['*']
                              volumes: ['*']
                              hostNetwork: true
                              hostPorts:
                              - min: 0
                                max: 65535
                              hostIPC: true
                              hostPID: true
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: RoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp-nodes
                              namespace: kube-system
                            roleRef:
                              kind: ClusterRole
                              name: privileged-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                            # Allows access to resources required by the kubelet component
                            - kind: Group
                              apiGroup: rbac.authorization.k8s.io
                              name: system:nodes
                            # Allows access to resources required by the kubelet user
                            - kind: User
                              apiGroup: rbac.authorization.k8s.io
                              name: kubelet # Legacy node ID
                            # bind all service accounts in namespace kube-system (kube-proxy, CNI)
                            - apiGroup: rbac.authorization.k8s.io
                              kind: Group
                              name: system:serviceaccounts:kube-system
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp
                            rules:
                            - apiGroups: ['policy']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['privileged']
                            - apiGroups: ['extensions']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['privileged']
                            EOF

                            sfKubectl apply -f /etc/kubernetes/kubeadm/psp-default.yaml || sfFail 216 "failure applying psp-default.yaml"
                            sfKubectl apply -f /etc/kubernetes/kubeadm/psp-privileged.yaml || sfFail 217 "failure applying psp-privileged.yaml"
                            {{ end }}

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 218 "failure checking apiserver"
                            sfRetry sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 219 "failure checking controller"
                            sfRetry sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 220 "failure checking scheduler"

                            echo "cp1 init done"
                            touch /etc/kubernetes/.cp1
                            sfExit

                    cni:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            case {{ .CNI }} in
                                flannel)
                                    # Apply Flannel yaml
                                    sfKubectl apply -f https://raw.githubusercontent.com/coreos/flannel/{{.FlannelVersion}}/Documentation/kube-flannel.yml || sfFail 221 "Error applying flannel"
                                    ;;
                                calico)
                                    # Download Calico yaml
                                    wget https://docs.projectcalico.org/manifests/calico.yaml -P ${SF_TMPDIR} || sfFail 222 "Error downloading calico yaml"

                                    # Configure Pods subnet
                                    sed -i 's/# - name: CALICO_IPV4POOL_CIDR/- name: CALICO_IPV4POOL_CIDR/g' ${SF_TMPDIR}/calico.yaml
                                    sed -i 's/#   value: "192.168.0.0\/16"/  value: "10.244.0.0\/16"/g' ${SF_TMPDIR}/calico.yaml

                                    {{ if eq .CalicoNetworking "vxlan" }}
                                    # Disable IP in IP
                                    sed -i '/.*- name: CALICO_IPV4POOL_IPIP/,//s/.*value: "Always"/              value: "Never"/' ${SF_TMPDIR}/calico.yaml
                                    # Enable VXLAN
                                    sed -i '/.*- name: CALICO_IPV4POOL_VXLAN/,//s/.*value: "Never"/              value: "Always"/' ${SF_TMPDIR}/calico.yaml
                                    # change calico backend
                                    sed -i 's/calico_backend: "bird"/calico_backend: "vxlan"/' ${SF_TMPDIR}/calico.yaml
                                    # update liveness and readyness
                                    sed -i '/.*- -bird-live/d' ${SF_TMPDIR}/calico.yaml
                                    sed -i '/.*- -bird-ready/d' ${SF_TMPDIR}/calico.yaml
                                    {{ end }}

                                    # Apply Calico yaml
                                    sfKubectl apply -f ${SF_TMPDIR}/calico.yaml || sfFail 223 "Error applying calico yaml"
                                    #rm ${SF_TMPDIR}/calico.yaml
                                    ;;
                            esac

                            sfExit 0

                    cpx-init:
                        targets:
                            masters: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit
                            [ -f /etc/kubernetes/.cp1 ] && sfExit

                            sfDropzonePop ${SF_TMPDIR} || sfFail 224 "failure creating drop zone"
                            sfDropzoneClean || sfFail 224 "failure cleaning drop zone"

                            [ -f ${SF_TMPDIR}/cp_join_cmd.sh ] || sfFail 225 "failure due to missing cp_join_cmd.sh"
                            # sometimes, etcd cluster appears to be unhealthy when calling this... So retries!
                            sfRetryEx {{ or .reserved_ClusterJoinTimeout "14m" }} {{ or .reserved_DefaultDelay 10 }} bash -x ${SF_TMPDIR}/cp_join_cmd.sh || sfFail 226 "failure running cp_join_cmd.sh"
                            [ -f ${SF_TMPDIR}/init_cluster_admin_kube.sh ] || sfFail 227 "failure due to missing init_cluster_admin_kube.sh"
                            bash -x ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 228 "failure running init_cluster_admin_kube.sh"

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 229 "failure running k8s apiserver"
                            sfRetry sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 230 "failure running k8s controller"
                            sfRetry sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 231 "failure running k8s scheduler"

                            touch /etc/kubernetes/.joined
                            echo "cpx init done"
                            sfExit

                    join-gws:
                        targets:
                            gateways: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            MASTERIP=
                            for m in {{ range .ClusterMasterIPs }}{{.}} {{ end -}}; do
                                op=-1
                                NODE_JOIN_CMD=$(sfRemoteExec $m kubeadm token create --print-join-command) && op=$? || true
                                [ $op -ne 0 ] && continue
                                NODE_JOIN_CMD=$(echo $NODE_JOIN_CMD | head -1)
                                MASTERIP=$m
                                break
                            done
                            [ -z "$MASTERIP" ] && echo "failed to find available master to register with. Aborted." && sfFail 232 "failed to find available master to register with. Aborted."
                            eval $NODE_JOIN_CMD || sfFail 233 "failed joining nodes"

                            sfRemoteExec $MASTERIP sudo -u {{ .ClusterAdminUsername }} -i kubectl taint nodes {{ .Hostname }} key=value:NoSchedule || sfFail 234 "failure checking tainted nodes"

                            touch /etc/kubernetes/.joined
                            sfExit

                    join-nodes:
                        targets:
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            MASTERIP=
                            for m in {{ range .ClusterMasterIPs }}{{.}} {{ end -}}; do
                                op=-1
                                NODE_JOIN_CMD=$(sfRemoteExec $m kubeadm token create --print-join-command) && op=$? || true
                                [ $op -ne 0 ] && continue
                                NODE_JOIN_CMD=$(echo $NODE_JOIN_CMD | head -1)
                                MASTERIP=$m
                                break
                            done
                            [ -z "$MASTERIP" ] && echo "failed to find available master to register with. Aborted." && sfFail 235 "failed to find available master to register with. Aborted."
                            eval $NODE_JOIN_CMD || sfFail 236 "failure joining nodes"

                            sfExit

                    fw:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            # add interfaces to trusted zone of the firewall
                            case {{.CNI}} in
                                flannel)
                                    sfFirewall --zone=trusted --add-interface=cni0 --permanent || sfFail 224 "failed to add cni0 interface to firewalld trusted zone"
                                    sfFirewall --zone=trusted --add-interface={{.CNI}}.1 --permanent || sfFail 225 "failed to add {{.CNI}}.1 interface to firewalld trusted zone"
                                    ;;
                                calico)
                                    sfFirewall --zone=trusted --add-interface=vxlan.calico --permanent || sfFail 225 "failed to add vxlan.calico interface to firewalld trusted zone"
                                    ;;
                            esac
                            sfFirewallReload || sfFail 226 "failed to reload firewalld configuration for cni driver {{.CNI}}"

                            sfExit

                    supplemental:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && echo "already joined, nothing to do" && sfExit

                            # Allows pods to start on master if there is only one master or if it's explicitely requested
                            if [ "{{.ClusterComplexity}}" = "small" -o "{{.AllowPodsOnMasters}}" = "true" ]; then
                                sfKubectl taint nodes --all node-role.kubernetes.io/master- || true
                            fi

                            # Adds namespace safescale
                            sfKubectl create namespace safescale
                            # FIXME: add label to namespace (in prevision of network policies)
                            # sfKubectl set label
                            # adds Kubernetes Dashboard
                            if [ "{{.Dashboard}}" == "true" ]; then
                                if curl --output /dev/null --silent --head --fail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                    sfRetry sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 237 "Unable to install kubernetes dashboard version $(sfGithubLastRelease)"
                                else
                                    if curl --output /dev/null --silent --head --fail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                        sfRetry sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 238 "Unable to install kubernetes dashboard version $(sfGithubLastNotBetaRelease)"
                                    fi
                                fi
                            fi
                            sfExit

                    final:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            [ ! -f /etc/kubernetes/.joined ] && touch /etc/kubernetes/.joined
                            sfExit

            remove:
                pace: node,reset,clean
                steps:
                    node:
                        targets:
                            masters: one
                        run: |
                            sfKubectl drain {{.Hostname}} --delete-local-data --force --ignore-daemonsets
                            sfKubectl delete node {{.Hostname}}
                            sfExit

                    reset:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            kubeadm reset -f
                            sfExit

                    clean:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfApt purge -y kubectl kubeadm kubelet || sfFail 192 "failure purging kubernetes"
                                    ;;
                                redhat|rhel|fedora|centos)
                                    yum remove -y kubectl kubeadm kubelet || sfFail 193 "failure purging kubernetes"
                                    ;;
                            esac
                            sfFirewallReload || sfFail 194 "Firewall problem"
                            rm -f /etc/kubernetes/.joined
                            sfExit

...
