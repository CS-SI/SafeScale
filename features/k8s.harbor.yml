# Copyright 2018-2020, CS Systemes d'Information, http://www.c-s.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
feature:
    suitableFor:
        host: no
        cluster: k8s,dcos

    requirements:
        features:
            - kubernetes
            - k8shelm

    parameters:
        - Environnement=prod

    install:
        bash:
            check:
                pace: helm_chart
                steps:
                    helm_chart:
                        targets:
                            node: none
                            masters: one
                        run: |
                            sudo -u cladm -i helm search ...

            add:
                pace: prepare
                steps:
                    prepare:
                        targets:
                            node: none
                            masters: one
                        run: |
                            mkdir -p ${SF_ETCDIR}/harbor
                            cd ${SF_ETCDIR}/harbor

                            cat << EOF > values.yaml
                            master:
                                affinity:
                                nodeAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    nodeSelectorTerms:
                                    - matchExpressions:
                                        - key: "node-role.kubernetes.io/worker"
                                        operator: In
                                        values:
                                        - S1PDGS
                                podAntiAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    - labelSelector:
                                        matchExpressions:
                                        - key: role
                                        operator: In
                                        values:
                                        - master
                                        - key: release
                                        operator: In
                                        values:
                                        - postgresql-infra
                                    topologyKey: "kubernetes.io/hostname"
                            slave:
                                affinity:
                                nodeAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    nodeSelectorTerms:
                                    - matchExpressions:
                                        - key: "node-role.kubernetes.io/worker"
                                        operator: In
                                        values:
                                        - S1PDGS
                                podAntiAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    - labelSelector:
                                        matchExpressions:
                                        - key: role
                                        operator: In
                                        values:
                                        - slave
                                        - key: release
                                        operator: In
                                        values:
                                        - postgresql-infra
                                    topologyKey: "kubernetes.io/hostname"
                            pgpool:
                                affinity:
                                nodeAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    nodeSelectorTerms:
                                    - matchExpressions:
                                        - key: "node-role.kubernetes.io/worker"
                                        operator: In
                                        values:
                                        - S1PDGS
                                podAntiAffinity:
                                    requiredDuringSchedulingIgnoredDuringExecution:
                                    - labelSelector:
                                        matchExpressions:
                                        - key: role
                                        operator: In
                                        values:
                                        - pgpool
                                        - key: release
                                        operator: In
                                        values:
                                        - postgresql-infra
                                    topologyKey: "kubernetes.io/hostname"
                            EOF
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfWaitForApt
                                    sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfApt install -y ebtables socat || sfFail 194
                                    ;;
                                redhat|rhel|centos|fedora)
                                    sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfYum install -y ebtables socat || sfFail 195
                                    cat >/etc/sysctl.d/kubernetes.conf <<-'EOF'
                            net.bridge.bridge-nf-call-ip6tables = 1
                            net.bridge.bridge-nf-call-iptables = 1
                            net.bridge.bridge-nf-call-arptables = 1
                            EOF
                                    modprobe br_netfilter &>/dev/null
                                    sysctl --system

                                    # Set SELinux in permissive mode (effectively disabling it)
                                    if [[ -n $(command -v getenforce) ]]; then
                                        act=0
                                        getenforce | grep "Disabled" || act=1
                                        if [ $act -eq 1 ]; then
                                            if [[ -n $(command -v setenforce) ]]; then
                                                setenforce 0 || sfFail 201 "Error setting selinux in Permissive mode"
                                                sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
                                            fi
                                        fi
                                    fi
                                    ;;
                                *)
                                    echo "Unmanaged Linux distribution '$LINUX_KIND'"
                                    sfFail 196
                                    ;;
                            esac
    
                            echo "# IPVS modules used by Kubernetes" >>/etc/modules
                            for i in ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh; do
                                echo $i >>/etc/modules
                                modprobe $i 2>/dev/null || sfFail 201 "Error enabling kernel module $i, error code $?"
                            done
                            
                            op=-1
                            for i in nf_conntrack_ipv4 nf_conntrack; do
                                echo $i >>/etc/modules
                                modprobe $i 2>/dev/null && op=0 || true
                            done
                            if [ $op -eq -1 ]; then
                                sfFail 201 "Error enabling kernel module nf_conntrack and nf_conntrack_ipv4"
                            fi

                            echo "net.ipv4.ip_forward=1" >>/etc/sysctl.d/99-sysctl.conf
                            sysctl --system

                            # Disable swap if enabled
                            op=-1
                            SWAPS=$(grep "swap[[:space:]]*sw[[:space:]]*" /etc/fstab | column -t | cut -d' ' -f1) && op=$? || true
                            if [ $op -eq 0 ]; then
                                cp /etc/fstab /etc/fstab.before_swap_disable
                                for s in $SWAPS; do
                                    swapoff $s &>/dev/null
                                    grep -v "$s" /etc/fstab >>/etc/fstab.new && mv /etc/fstab.new /etc/fstab
                                done
                            fi
                            sfExit

                    prepare-host:
                        targets:
                            masters: all
                            nodes: all
                        run: |
                            if [ "{{.Hardening}}" == "true" ]; then

                                case $(sfGetFact "linux_kind") in
                                    debian|ubuntu)
                                        sfRetry 3m 5 "sfApt update"
                                        sfRetry 3m 5 "sfApt install -y audit audit-libs quota" || sfFail 192
                                        ;;

                                    centos|fedora|redhat|rhel)
                                        sfYum -y install audit audit-libs quota || sfFail 192
                                        ;;

                                    *) echo "unsupported linux distribution ''"
                                    sfFail 193
                                esac

                                ## Configures ulimit
                                cat >> /etc/security/limits.conf <<EOF
                            *      soft      nproc      100
                            *      hard      nproc      200
                            *      soft      nofile      1024
                            *      hard      nofile      2048
                            EOF

                                ## Configures audit rules
                                cat > /etc/audit/rules.d/docker.rules <<EOF
                            -w /usr/bin/docker -p awx -k docker_binary
                            -w /var/lib/docker -p aw -k docker_data
                            -w /etc/docker -p aw -k docker_configuration
                            -w /etc/docker/daemon.json -p aw -k docker_daemon
                            -w /etc/default/docker -p aw -k docker_default
                            -w /usr/bin/containerd -p awx -k docker_containerd
                            -w /usr/bin/runc -p awx -k docker_runc
                            EOF
                                sfService restart auditd

                            fi
                            sfExit

                    common-tools:
                        targets:
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            case $(sfGetFact "linux_kind") in
                                debian|ubuntu)
                                    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
                                    cat >/etc/apt/sources.list.d/kubernetes.list <<-EOF
                            deb https://apt.kubernetes.io/ kubernetes-xenial main
                            EOF
                                    sfRetry 3m 5 "sfApt update"
                                    sfRetry 3m 5 "sfApt install -y kubelet kubeadm kubectl" || sfFail 192
                                    apt-mark hold kubelet kubeadm kubectl
                                    ;;

                                centos|fedora|redhat|rhel)
                                    cat >/etc/yum.repos.d/kubernetes.repo <<-EOF
                            [kubernetes]
                            name=Kubernetes
                            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
                            enabled=1
                            gpgcheck=1
                            repo_gpgcheck=1
                            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
                            exclude=kube*
                            EOF

                                    sfYum -y install kubelet-{{.KubeVersion}} kubeadm-{{.KubeVersion}} kubectl-{{.KubeVersion}} --disableexcludes=kubernetes || sfFail 192
                                    ;;

                                *) echo "unsupported linux distribution ''"
                                   sfFail 193
                            esac
                            sfService enable kubelet && sfService start kubelet
                            sfExit

                    reverseproxy:
                        targets:
                            gateways: all
                        run: |
                            if [ -d ${SF_ETCDIR}/edgeproxy4network ]; then
                                TARGET_DIR=${SF_ETCDIR}/edgeproxy4network
                            elif [ -d ${SF_ETCDIR}/kong4gateway ]; then
                                TARGET_DIR=${SF_ETCDIR}/kong4gateway
                            fi
                            [ -z "${TARGET_DIR+x}" ] && sfFail 197

                            cat >${TARGET_DIR}/includes/kubernetes.conf <<-'EOF'
                            upstream kubernetes-api-cluster {
                                {{range .ClusterMasterIPs}}server {{.}}:6443;
                                {{end}}
                            }

                            server {
                                listen 6443;
                                proxy_pass kubernetes-api-cluster;
                            }
                            EOF
                            sfReverseProxyReload
                            sfExit

                    cp1-init:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            mkdir -p /etc/kubernetes/kubeadm

                            cat >/etc/kubernetes/kubeadm/kubeadm-config.yaml <<-'EOF'
                            ---
                            apiVersion: kubeadm.k8s.io/v1beta1
                            kind: ClusterConfiguration
                            kubernetesVersion: stable
                            controlPlaneEndpoint: "{{.DefaultRouteIP}}:6443"
                            kubernetesVersion: v{{.KubeVersion}}
                            etcd:
                              local:
                                dataDir: /var/lib/etcd
                            networking:
                              podSubnet: 10.244.0.0/16
                            EOF

                            # apiVersion: kubeadm.k8s.io/v1beta1
                            # bootstrapTokens:
                            # - groups:
                            #   - system:bootstrappers:kubeadm:default-node-token
                            #   ttl: 24h0m0s
                            #   usages:
                            #   - signing
                            #   - authentication
                            # kind: InitConfiguration
                            # localAPIEndpoint:
                            #   advertiseAddress: {{ .HostIP }}
                            #   bindPort: 6443
                            # nodeRegistration:
                            #   criSocket: /var/run/dockershim.sock
                            #   name: {{ .Hostname }}
                            #   taints:
                            #   - effect: NoSchedule
                            #     key: node-role.kubernetes.io/master
                            # ---
                            # apiServer:
                            #   timeoutForControlPlane: 4m0s
                            #   extraArgs:
                            #     authorization-mode: RBAC,Node
                            #     audit-log-path: /var/log/k8s_audit.log
                            #     audit-log-maxage: "30"
                            #     audit-log-maxbackup: "10"
                            #     audit-log-maxsize: "100"
                            #     service-account-lookup: "true"
                            #     profiling: "false"
                            # apiVersion: kubeadm.k8s.io/v1beta1
                            # certificatesDir: /etc/kubernetes/pki
                            # clusterName: kubernetes
                            # controlPlaneEndpoint: "{{ .DefaultRouteIP }}"
                            # controllerManager:
                            #   extraArgs:
                            #     bind-address: "{{ .HostIP }}"
                            #     terminated-pod-gc-threshold: "12500"
                            #     profiling: "false"
                            #     use-service-account-credentials: "true"
                            #     feature-gates: RotateKubeletServerCertificate=true
                            # dns:
                            #   type: CoreDNS
                            # etcd:
                            #   local:
                            #     dataDir: /var/lib/etcd
                            # imageRepository: k8s.gcr.io
                            # kind: ClusterConfiguration
                            # kubernetesVersion: v{{.KubeVersion}}
                            # networking:
                            #   dnsDomain: cluster.local
                            #   podSubnet: 10.244.0.0/16
                            #   serviceSubnet: 10.96.0.0/12
                            # scheduler: {}
                            # EOF

                            # if [ "$(sfGetFact "redhat_like")" == "1" ]; then
                            #     cat >/etc/sysconfig/kubelet <<-EOF
                            # KUBELET_EXTRA_ARGS="--hostname-override {{ .HostIP }}
                            # EOF
                            # fi

                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} kubeadm config images pull || sfFail 198
                            kubeadm init --config=/etc/kubernetes/kubeadm/kubeadm-config.yaml || sfFail 199
                            cp_join_cmd=$(kubeadm token create --ttl 10m --print-join-command) || sfFail 200
                            cert_key=$(kubeadm init phase upload-certs --experimental-upload-certs | tail -n 1) || sfFail 201

                            cat >${SF_TMPDIR}/init_cluster_admin_kube.sh <<-'EOF'
                            mkdir -p ~{{.Username}}/.kube
                            cp -f /etc/kubernetes/admin.conf ~{{.Username}}/.kube/config
                            chown -R {{.Username}}:{{.Username}} ~{{.Username}}/.kube && \
                            chmod -R go-rwx ~{{.Username}}/.kube

                            sudo chown root:{{.Username}} /etc/kubernetes/pki/etcd/;
                            sudo chown root:{{.Username}} /etc/kubernetes/pki/etcd/ca.crt;
                            sudo chown root:{{.Username}} /etc/kubernetes/pki/etcd/server.key;
                            sudo chmod g+r /etc/kubernetes/pki/etcd/server.key;
                            sudo chown root:{{.Username}} /etc/kubernetes/pki/etcd/server.crt;
                            EOF

                            # execute init_cluster_admin_kube.sh on the current master
                            bash ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 202
                            # Starting from here, any kubectl command must be changed to sfKubectl

                            # Push control-plane join command to all the other masters
                            echo "$cp_join_cmd --experimental-control-plane --certificate-key $cert_key" >${SF_TMPDIR}/cp_join_cmd.sh
                            cat ${SF_TMPDIR}/cp_join_cmd.sh
                            sfDropzonePush ${SF_TMPDIR}/cp_join_cmd.sh || fail 203
                            sfDropzonePush ${SF_TMPDIR}/init_cluster_admin_kube.sh || fail 204
                            for ip in {{range .ClusterMasterIPs}}{{.}} {{end}}; do
                                [ "$ip" = "{{.HostIP}}" ] && continue
                                sfDropzoneSync $ip || sfFail 205
                            done
                            rm ${SF_TMPDIR}/cp_join_cmd.sh ${SF_TMPDIR}/init_cluster_admin_kube.sh
                            sfDropzoneClean

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 206
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 207
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 208

                            touch /etc/kubernetes/.joined
                            echo "cp1 init done"

                            if [ "{{.Hardening}}" == "true" ]; then
                                cat >/etc/kubernetes/kubeadm/psp-default.yaml <<-'EOF'
                            ---
                            apiVersion: extensions/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: default
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
                                seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
                            spec:
                              privileged: false
                              allowPrivilegeEscalation: false
                              allowedCapabilities: []
                              volumes:
                                - 'configMap'
                                - 'emptyDir'
                                - 'projected'
                                - 'secret'
                                - 'downwardAPI'
                                - 'persistentVolumeClaim'
                              hostNetwork: false
                              hostIPC: false
                              hostPID: false
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            rules:
                              - apiGroups: ['policy']
                                resources: ['podsecuritypolicies']
                                verbs: ['use']
                                resourceNames: ['default']
                              - apiGroups: ['extensions']
                                resources: ['podsecuritypolicies']
                                verbs: ['use']
                                resourceNames: ['default']
                            ---
                            kind: ClusterRoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            roleRef:
                              kind: ClusterRole
                              name: default-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                              - kind: Group
                                name: system:authenticated
                                apiGroup: rbac.authorization.k8s.io
                            EOF

                            cat >/etc/kubernetes/kubeadm/psp-privileged.yaml <<-'EOF'
                            ---
                            apiVersion: extensions/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: privileged
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
                            spec:
                              privileged: true
                              allowPrivilegeEscalation: true
                              allowedCapabilities: ['*']
                              volumes: ['*']
                              hostNetwork: true
                              hostPorts:
                                - min: 0
                                  max: 65535
                              hostIPC: true
                              hostPID: true
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: RoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp-nodes
                              namespace: kube-system
                            roleRef:
                              kind: ClusterRole
                              name: privileged-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                              # Allows access to resources required by the kubelet component
                              - kind: Group
                                apiGroup: rbac.authorization.k8s.io
                                name: system:nodes
                              # Allows access to resources required by the kubelet user
                              - kind: User
                                apiGroup: rbac.authorization.k8s.io
                                name: kubelet # Legacy node ID
                              # bind all service accounts in namespace kube-system (kube-proxy, CNI)
                              - apiGroup: rbac.authorization.k8s.io
                                kind: Group
                                name: system:serviceaccounts:kube-system
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp
                            rules:
                              - apiGroups: ['policy']
                                resources: ['podsecuritypolicies']
                                verbs: ['use']
                                resourceNames: ['privileged']
                              - apiGroups: ['extensions']
                                resources: ['podsecuritypolicies']
                                verbs: ['use']
                                resourceNames: ['privileged']
                            EOF

                                # Apply policies
                                sfKubectl apply -f /etc/kubernetes/kubeadm/psp-default.yaml || sfFail 209
                                sfKubectl apply -f /etc/kubernetes/kubeadm/psp-privileged.yaml || sfFail 210
                            fi

                            sfExit

                    cni:
                        targets:
                            masters: one
                        run: |
                            case {{.CNI}} in
                                flannel) # To be removed
                                    sfKubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml || sfFail 209

                                    sfFirewallAdd --zone=trusted --add-interface=flannel.1
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 204 "Firewall problem"
                                    ;;
                                calico)
                                    # Should already exist...
                                    mkdir -p /etc/kubernetes/kubeadm/

                                    # Applies RBAC Calico
                                    wget https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-calico.yaml -P /etc/kubernetes/kubeadm/
                                    sfKubectl apply -f /etc/kubernetes/kubeadm/rbac-kdd-calico.yaml || sfFail 201

                                    # Downloads default calico
                                    wget https://docs.projectcalico.org/v3.10/manifests/calico.yaml -P /etc/kubernetes/kubeadm/

                                    # Creates our patch file
                                    cat >/etc/kubernetes/kubeadm/calico.yaml.patch <<-'EOF'
                            --- calico.yaml	2019-11-21 17:06:25.590230285 +0100
                            +++ calico.yaml.new	2019-11-21 17:02:11.885017690 +0100
                            @@ -24,7 +24,7 @@
                                    "plugins": [
                                        {
                                        "type": "calico",
                            -          "log_level": "info",
                            +          "log_level": "debug",
                                        "datastore_type": "kubernetes",
                                        "nodename": "__KUBERNETES_NODE_NAME__",
                                        "mtu": __CNI_MTU__,
                            @@ -516,7 +516,7 @@
                                        # It can be deleted if this is a fresh installation, or if you have already
                                        # upgraded to use calico-ipam.
                                        - name: upgrade-ipam
                            -          image: calico/cni:v3.10.1
                            +          image: calico/cni:v3.10.0
                                        command: ["/opt/cni/bin/calico-ipam", "-upgrade"]
                                        env:
                                            - name: KUBERNETES_NODE_NAME
                            @@ -536,7 +536,7 @@
                                        # This container installs the CNI binaries
                                        # and CNI network config file on each node.
                                        - name: install-cni
                            -          image: calico/cni:v3.10.1
                            +          image: calico/cni:v3.10.0
                                        command: ["/install-cni.sh"]
                                        env:
                                            # Name of the CNI config file to create.
                            @@ -570,7 +570,7 @@
                                        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
                                        # to communicate with Felix over the Policy Sync API.
                                        - name: flexvol-driver
                            -          image: calico/pod2daemon-flexvol:v3.10.1
                            +          image: calico/pod2daemon-flexvol:v3.10.0
                                        volumeMounts:
                                        - name: flexvol-driver-host
                                            mountPath: /host/driver
                            @@ -579,7 +579,7 @@
                                        # container programs network policy and routes on each
                                        # host.
                                        - name: calico-node
                            -          image: calico/node:v3.10.1
                            +          image: calico/node:v3.10.0
                                        env:
                                            # Use Kubernetes API as the backing datastore.
                                            - name: DATASTORE_TYPE
                            @@ -606,6 +606,8 @@
                                            value: "autodetect"
                                            # Enable IPIP
                                            - name: CALICO_IPV4POOL_IPIP
                            +              value: "Never"
                            +            - name: CALICO_IPV4POOL_VXLAN
                                            value: "Always"
                                            # Set MTU for tunnel device used if ipip is enabled
                                            - name: FELIX_IPINIPMTU
                            @@ -617,7 +619,7 @@
                                            # chosen from this range. Changing this value after installation will have
                                            # no effect. This should fall within `--cluster-cidr`.
                                            - name: CALICO_IPV4POOL_CIDR
                            -              value: "192.168.0.0/16"
                            +              value: "10.244.0.0/16"
                                            # Disable file logging so `kubectl logs` works.
                                            - name: CALICO_DISABLE_FILE_LOGGING
                                            value: "true"
                            @@ -753,7 +755,7 @@
                                    priorityClassName: system-cluster-critical
                                    containers:
                                        - name: calico-kube-controllers
                            -          image: calico/kube-controllers:v3.10.1
                            +          image: calico/kube-controllers:v3.10.0
                                        env:
                                            # Choose which controllers to run.
                                            - name: ENABLED_CONTROLLERS
                            EOF

                                    # Installs the 'patch' command
                                    case $(sfGetFact "linux_kind") in
                                        debian|ubuntu)
                                            sfRetry 3m 5 "sfApt update"
                                            sfRetry 3m 5 "sfApt install -y patch" || sfFail 202
                                            ;;

                                        centos|fedora|redhat|rhel)
                                            sfYum -y install patch || sfFail 203
                                            ;;

                                        *) echo "unsupported linux distribution ''"
                                        sfFail 204
                                    esac

                                    # Patches calico file
                                    patch -l /etc/kubernetes/kubeadm/calico.yaml /etc/kubernetes/kubeadm/calico.yaml.patch

                                    # Applies patched file
                                    sfKubectl apply -f /etc/kubernetes/kubeadm/calico.yaml || sfFail 205

                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 206 "Firewall problem"
                                    ;;
                                canal) # To be removed
                                    sfKubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/canal/canal.yaml || sfFail 211
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 204 "Firewall problem"
                                    ;;
                            esac
                            sfExit 0

                    cpx-init:
                        targets:
                            masters: all
                        run: |
                            # Don't try to init a kubernetes cluster already running
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            sfDropzonePop ${SF_TMPDIR} || sfFail 212
                            sfDropzoneClean

                            [ -f ${SF_TMPDIR}/cp_join_cmd.sh ] || sfFail 213
                            bash ${SF_TMPDIR}/cp_join_cmd.sh || sfFail 214
                            [ -f ${SF_TMPDIR}/init_cluster_admin_kube.sh ] || sfFail 215
                            bash ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 216

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 217
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 218
                            sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 219

                            touch /etc/kubernetes/.joined
                            echo "cpx init done"

                            # Configure firewall
                            case {{.CNI}} in
                                flannel)
                                    sfFirewallAdd --zone=trusted --add-interface=flannel.1
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 204 "Firewall problem"
                                    ;;
                                calico)
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 204 "Firewall problem"
                                canal)
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 204 "Firewall problem"
                                    ;;
                            esac
                            sfExit

                    join:
                        targets:
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            MASTERIP=
                            for m in {{ range .ClusterMasterIPs }}{{.}} {{ end -}}; do
                                op=-1
                                NODE_JOIN_CMD=$(sfRemoteExec $m kubeadm token create --print-join-command) && op=$? || true
                                [ $op -ne 0 ] && continue
                                NODE_JOIN_CMD=$(echo $NODE_JOIN_CMD | head -1)
                                MASTERIP=$m
                                break
                            done
                            [ -z "$MASTERIP" ] && echo "failed to find available master to register with. Aborted." && sfFail 220
                            eval $NODE_JOIN_CMD || sfFail 221
                            touch /etc/kubernetes/.joined
                            sfExit

                    final:
                        targets:
                            masters: one
                        run: |
                            # Allows pods to start on master if there is only one master or if it's explicitely requested
                            [ "{{.ClusterComplexity}}" = "small" -o "{{.AllowPodsOnMasters}}" = "true" ] && sfKubectl taint nodes --all node-role.kubernetes.io/master- || true
                            # adds Kubernetes Dashboard
                            if [ "{{.Dashboard}}" == "true" ]; then
                                if curl --output /dev/null --silent --head --fail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                    sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 222 "Unable to install kubernetes dashboard version $(sfGithubLastRelease)"
                                else
                                    if curl --output /dev/null --silent --head --fail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                        sfRetry {{.TemplateOperationTimeout}} {{.TemplateOperationDelay}} sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 222 "Unable to install kubernetes dashboard version $(sfGithubLastNotBetaRelease)"
                                    fi
                                fi
                            fi

                            # cat <<EOF | kubectl apply -f -
                            # apiVersion: v1
                            # kind: ServiceAccount
                            # metadata:
                            #   name: cladm
                            #   namespace: kube-system
                            # ---
                            # apiVersion: rbac.authorization.k8s.io/v1
                            # kind: ClusterRoleBinding
                            # metadata:
                            #   name: cladm
                            # roleRef:
                            #   apiGroup: rbac.authorization.k8s.io
                            #   kind: ClusterRole
                            #   name: cluster-admin
                            # subjects:
                            #   - kind: ServiceAccount
                            #     name: cladm
                            #     namespace: kube-system
                            # EOF
                            sfExit

            remove:
                pace: node,reset,reverseproxy,clean
                steps:
                    node:
                        targets:
                            masters: one
                        run: |
                            sfKubectl drain {{.Hostname}} --delete-local-data --force --ignore-daemonsets
                            sfKubectl delete node {{.Hostname}}

                    reverseproxy:
                        targets:
                            gateways: all
                        run: |
                            rm -f /etc/kong4gateway/includes/kubernetes.conf
                            sfReverseProxyReload

                    reset:
                        targets:
                            masters: all
                            nodes: all
                        run: |
                            kubeadm reset -f

                    clean:
                        targets:
                            masters: all
                            nodes: all
                        run: |
                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfApt purge -y kubectl-{{.KubeVersion}} kubeadm-{{.KubeVersion}} kubelet-{{.KubeVersion}} || sfFail 223 "exiture purging kubernetes"
                                    ;;
                                redhat|rhel|fedora|centos)
                                    sfYum remove -y kubectl-{{.KubeVersion}} kubeadm-{{.KubeVersion}} kubelet-{{.KubeVersion}} || sfFail 224 "exiture purging kubernetes"
                                    ;;
                            esac
                            sfFirewallReload || sfFail 204 "Firewall problem"
                            rm -f /etc/kubernetes/.joined
                            sfExit

...
