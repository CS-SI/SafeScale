
# Copyright 2018-2021, CS Systemes d'Information, http://csgroup.eu
#
# Licensed under the Apache License, SparkRelease 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

---
feature:
    suitableFor:
        cluster: boh

    requirements:
        - docker-swarm

    parameters:
        - SparkRelease=2.3.2
        - ConsolePort=63013

    install:
        bash:
            check:
                pace: image
                steps:
                    image:
                        targets:
                            masters: all
                        run: |
                            docker image ls {{ "--format '{{.Repository}}:{{.Tag}}'" }} | grep 'safescale/sparkmaster:{{ .SparkRelease }}' &>/dev/null || sfFail 192
                            sfExit

            add:
                pace: image,stack,start
                steps:
                    image:
                        targets:
                            masters: all
                        run: |
                            cat "sparkmaster\t{{ .ConsolePort }}/tcp\t# Spark Master console" >>/opt/safescale/etc/services

                            mkdir -p ${SF_ETCDIR}/sparkmaster4platform/build

                            cat >${SF_ETCDIR}/sparkmaster4platform/build/spark.sh <<-'EOF'
                            export SPARK_HOME=/usr/local/spark
                            export PYSPARK_DRIVER_PYTHON=python3
                            export PYSPARK_PYTHON=python3
                            export SPARK_MASTER_WEBUI_PORT=8081
                            export _PYSPARK_DRIVER_CONN_INFO_PATH=/tmp/pyspark.conn
                            EOF

                            cat >${SF_ETCDIR}/sparkmaster4platform/build/startup.sh <<-'EOF'
                            #!/bin/bash
                            source /etc/profile.d/spark.sh

                            export SPARK_NO_DAEMONIZE=

                            # start spark
                            exec $SPARK_HOME/sbin/start-master.sh
                            EOF

                            cat >${SF_ETCDIR}/sparkmaster4platform/build/Dockerfile <<-'EOF'
                            FROM alpine
                            LABEL maintainer="CS SI"

                            ENV DEBIAN_FRONTEND noninteractive

                            # -----------------
                            # Standard packages
                            # -----------------
                            RUN apk update \
                             && apk add bash openjdk8-jre python3 wget

                            # -----------------------------------------------
                            # Install tini, tiny init script built for docker
                            # -----------------------------------------------
                            ARG TINI_VERSION=v0.18.0
                            ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static /tini
                            RUN chmod u+rx /tini

                            # -------------------
                            # SPARK installation
                            # -------------------
                            ENV SPARK_VERSION={{ .SparkRelease }}
                            WORKDIR /usr/local/
                            RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz -O spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \
                            && tar -xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \
                            && rm -f spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \
                            && ln -s /usr/local/spark-${SPARK_VERSION}-bin-hadoop2.7 /usr/local/spark

                            # -----------------------------
                            # Set variable environnement
                            # -----------------------------
                            ENV SHELL /bin/bash

                            COPY spark.sh /etc/profile.d/
                            RUN chmod +x /etc/profile.d/spark.sh

                            RUN echo "source /etc/profile.d/spark.sh" >>/root/.bashrc

                            # ------------------------------
                            # Installs and configure startup
                            # ------------------------------
                            WORKDIR /opt
                            ADD startup.sh ./
                            RUN chmod u+x startup.sh

                            ENTRYPOINT [ "/tini", "--", "/opt/startup.sh" ]
                            EOF
                            sfRetry 15m 5 docker build --network host -t safescale/sparkmaster:{{ .SparkRelease }} ${SF_ETCDIR}/sparkmaster4platform/build || sfFail 192
                            sfExit

                    stack:
                        targets:
                            masters: all
                        run: |
                            cat >${SF_ETCDIR}/sparkmaster4platform/stack.yml <<-EOF
                            version: "3.7"

                            networks:
                                net:
                                    name: spark_net
                                    driver: overlay

                            services:
                                master:
                                    image: safescale/sparkmaster:{{ .SparkRelease }}
                                    networks:
                                        net:
                                            aliases:
                                                - sparkmaster
                                    ports:
                                        - published: {{ .ConsolePort }}
                                          target: 8081
                                          mode: host
                                        # - published: 7077
                                        #   target: 7077
                                    deploy:
                                        mode: replicated
                                        replicas: 1
                                        placement:
                                            constraints:
                                                - node.role == manager
                                        restart_policy:
                                            condition: on-failure
                                            delay: 5s
                                            max_attempts: 3
                                            window: 120s
                            EOF
                            sfExit

                    start:
                        targets:
                            masters: any
                        run: |
                            docker stack up -c ${SF_ETCDIR}/sparkmaster4platform/stack.yml sparkmaster4platform || sfFail 193
                            sfExit

                    ready:
                        targets:
                            masters: any
                        run: |
                            sfDoesDockerStackRun sparkmaster4platform || sfFail 194
                            sfExit

            remove:
                pace: stack,cleanup
                steps:
                    stack:
                        targets:
                            masters: any
                        run: |
                            docker stack rm sparkmaster4platform || sfFail 192
                            sfExit

                    cleanup:
                        targets:
                            masters: all
                        run: |
                            docker image rm -f safescale/sparkmaster:{{ .SparkRelease }} || sfFail 193
                            rm -rf ${SF_ETCDIR}/sparkmaster4platform
                            sfExit

    proxy:
        rules:
            - name: sparkmaster4platform_backend
              type: upstream
              targets:
                  masters: all
              content: |
                  {
                      "target": "{{ .HostIP }}:{{ .ConsolePort }}",
                      "weight": 100
                  }

            - name: sparkmaster4platform_svc
              type: service
              targets:
                  masters: one
              content: |
                  {
                      "protocol": "http",
                      "host": "sparkmaster4platform_backend"
                  }

            - name: sparkmaster4platform_route
              type: route
              targets:
                  masters: one
              content: |
                  {
                      "paths": [ "/_platform/sparkmaster/" ],
                      "strip_path": true,
                      "service": { "id": "{{ .sparkmaster4platform_svc }}" },
                      "source-control": {
                          "whitelist": [ "{{ .CIDR }}", "{{ .EndpointIP }}", "127.0.0.1" ]
                      }
                  }

...
