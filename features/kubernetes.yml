# Copyright 2018-2019, CS Systemes d'Information, http://www.c-s.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
feature:
    suitableFor:
        host: no
        cluster: all

    requirements:
        features:
            - docker

        # "clusterSizing" not honored currently
        clusterSizing:
            dcos:
                small:
                    nodes: "count >= 2, cpu >= 2"   # 1 master, 1 node
                normal:
                    nodes: "count >= 4, cpu >= 2"   # 3 masters, 1 node
                large:
                    nodes: "count >= 6, cpu >= 2 "  # 3 masters, 3 nodes
            boh:
                small:
                    masters: "count >= 1, cpu >= 2"
                    nodes:   "count >= 1, cpu >= 2"
                normal:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 1, cpu >= 2"
                large:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 3, cpu >= 2"
            k8s:
                small:
                    masters: "cpu >= 2"
                    nodes:   "cpu >= 2"
                normal:
                    masters: "count >= 3, cpu >= 2"
                    nodes:   "count >= 3, cpu >= 2"
                large:
                    masters: "count >= 5, cpu >= 2"
                    nodes:   "count >= 8, cpu >= 2"

    parameters:
        - AllowPodsOnMasters=false
        - AllowPodsOnGateways=false
        - CNI=calico
        - Dashboard=true
        - Hardening=true
        - KubeVersion=1.14.1
        - HelmVersion=2.14.1
        - HelmDefaultNamespace=default

    install:
        dcos:
            check:
                pace: deploy
                steps:
                    deploy:
                        targets:
                            masters: one
                        run: |
                            sfDcos kubernetes &>/dev/null
                            sfExit

            add:
                #pace: package,cli+config
                pace: package,cli,config
                steps:
                    package:
                        targets:
                            masters: one
                        options:
                            small: |
                                {
                                    "kubernetes": {
                                        "node_count": 1,
                                        "reserved_resources": {
                                            "kube_cpus": 1,
                                            "kube_mem": 1024,
                                            "kube_disk": 512
                                        }
                                    }
                                }
                            normal: |
                                {
                                    "kubernetes": {
                                        "high_availability": true,
                                        "node_count": 1,
                                        "reserved_resources": {
                                            "kube_cpus": 1,
                                            "kube_mem": 1024,
                                            "kube_disk": 512
                                        }
                                    }
                                }
                            large: |
                                {
                                    "kubernetes": {
                                        "high_availability": true,
                                        "node_count": 3,
                                        "reserved_resources": {
                                            "kube_cpus": 1,
                                            "kube_mem": 1024,
                                            "kube_disk": 512
                                        }
                                    }
                                }
                        run: |
                            op=-1
                            output=$(sfDcos package install --yes kubernetes {{.options}} 2>&1) && op=$? || true
                            [ $op -ne 0 ] && {
                                echo $output
                                echo $output | grep "already installed" &>/dev/null || sfFail $op
                            }
                            sfExit

                    cli:
                        targets:
                            masters: all
                        run: |
                            sfDcos package install --yes kubernetes --cli
                            sfExit

                    config:
                        targets:
                            masters: all
                        run: |
                            sfRetry 10m 20 sfDcos kubernetes plan show deploy --json | jq .status | grep COMPLETE &>/dev/null || sfFail 192
                            sfDcos kubernetes kubeconfig --apiserver-url https://apiserver.kubernetes.l4lb.thisdcos.directory:6443 && \
                            sfDcos config set-cluster kubernetes --server https://apiserver.kubernetes.l4lb.thisdcos.directory:6443
                            sfExit

            remove:
                pace: package
                steps:
                    package:
                        targets:
                            masters: one
                        run: |
                            sfDcos package remove --yes kubernetes || sfFail 192
                            sfExit

        bash:
            check:
                pace: masters,others
                steps:
                    others:
                        targets:
                            gateway: all
                            nodes: all
                        run: |
                            if [ -f /etc/kubernetes/.joined ]; then
                                pidof kubelet &>/dev/null || sfFail 192
                                sfExit
                            fi
                            sfFail 193

                    masters:
                        targets:
                            masters: all
                        run: |
                            if [ -f /etc/kubernetes/.joined ]; then
                                [ $(sfKubectl get nodes | wc -l) -gt 1 ] || sfFail 194
                                [ -f /usr/local/sbin/helm || sfFail 195
                                sfExit
                            fi
                            sfFail 196

            add:
                pace: sysconf,syshardening,common-tools,reverseproxy,cp1-init,cni,cpx-init,join,helm-certs,helm-binary,helm-rbac-files,helm-rbac-apply,helm-init,helm-sync,final
                steps:
                    sysconf:
                        targets:
                            gateway: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfWaitForApt
                                    sfRetry 5m 5 sfApt install -y ebtables socat || sfFail 192
                                    ;;
                                redhat|centos)
                                    sfRetry 5m 5 yum install -y ebtables socat || sfFail 193
                                    cat >/etc/sysctl.d/kubernetes.conf <<-'EOF'
                            net.bridge.bridge-nf-call-ip6tables = 1
                            net.bridge.bridge-nf-call-iptables = 1
                            net.bridge.bridge-nf-call-arptables = 1
                            EOF
                                    modprobe br_netfilter &>/dev/null
                                    sysctl --system

                                    # Set SELinux in permissive mode (effectively disabling it)
                                    if [[ -n $(command -v getenforce) ]]; then
                                        act=0
                                        getenforce | grep "Disabled" || act=1
                                        if [ $act -eq 1 ]; then
                                            if [[ -n $(command -v setenforce) ]]; then
                                                setenforce 0 || sfFail 201 "Error setting selinux in Permissive mode"
                                                sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
                                            fi
                                        fi
                                    fi
                                    ;;
                                *)
                                    echo "Unmanaged Linux distribution '$LINUX_KIND'"
                                    sfFail 194
                                    ;;
                            esac

                            echo "# IPVS modules used by Kubernetes" >>/etc/modules
                            for i in ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4; do
                                echo $i >>/etc/modules
                                modprobe $i
                            done

                            echo "net.ipv4.ip_forward=1" >>/etc/sysctl.d/99-sysctl.conf
                            sysctl --system

                            # Disable swap if enabled
                            op=-1
                            SWAPS=$(grep "swap[[:space:]]*sw[[:space:]]*" /etc/fstab | column -t | cut -d' ' -f1) && op=$? || true
                            if [ $op -eq 0 ]; then
                                cp /etc/fstab /etc/fstab.before_swap_disable
                                for s in $SWAPS; do
                                    swapoff $s &>/dev/null
                                    grep -v "$s" /etc/fstab >>/etc/fstab.new && mv /etc/fstab.new /etc/fstab
                                done
                            fi
                            sfExit

                    syshardening:
                        targets:
                            gateway: all
                            masters: all
                            nodes: all
                        run: |
                            {{ if eq .Hardening "true" }}
                            case $(sfGetFact "linux_kind") in
                                debian|ubuntu)
                                    sfApt update && sfApt install -y auditd quota quotatool || sfFail 193
                                    ;;

                                centos|redhat)
                                    yum -y install audit audit-libs quota || sfFail 193
                                    ;;

                                *) echo "unsupported linux distribution ''"
                                   sfFail 193
                                   ;;
                            esac

                            ## Configures ulimit - not compatible with a host on which a user graphical system is used - like remote desktop...
                            # cat >> /etc/security/limits.conf <<EOF
                            # *      soft      nproc      100
                            # *      hard      nproc      200
                            # *      soft      nofile      1024
                            # *      hard      nofile      2048
                            # EOF

                            ## Configures audit rules
                            cat >/etc/audit/rules.d/docker.rules <<EOF
                            -w /usr/bin/docker -p awx -k docker_binary
                            -w /var/lib/docker -p aw -k docker_data
                            -w /etc/docker -p aw -k docker_configuration
                            -w /etc/docker/daemon.json -p aw -k docker_daemon
                            -w /etc/default/docker -p aw -k docker_default
                            -w /usr/bin/containerd -p awx -k docker_containerd
                            -w /usr/bin/runc -p awx -k docker_runc
                            EOF
                            sfService restart auditd
                            {{ end }}
                            sfExit

                    common-tools:
                        targets:
                            gateway: all
                            masters: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            case $(sfGetFact "linux_kind") in
                                debian|ubuntu)
                                    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
                                    cat >/etc/apt/sources.list.d/kubernetes.list <<-EOF
                            deb https://apt.kubernetes.io/ kubernetes-xenial main
                            EOF
                            {{ if .KubeVersion }}
                                    sfApt update && sfApt install -y kubelet={{.KubeVersion}}-00 kubeadm={{.KubeVersion}}-00 kubectl={{.KubeVersion}}-00 || sfFail 199
                            {{ else }}
                                    sfApt update && sfApt install -y kubelet kubeadm kubectl || sfFail 199
                            {{ end }}
                                    apt-mark hold kubelet kubeadm kubectl
                                    ;;

                                centos|redhat)
                                    cat >/etc/yum.repos.d/kubernetes.repo <<-EOF
                            [kubernetes]
                            name=Kubernetes
                            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
                            enabled=1
                            gpgcheck=1
                            repo_gpgcheck=1
                            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
                            exclude=kube*
                            EOF
                            {{ if .KubeVersion }}
                                    yum -y install kubelet-{{.KubeVersion}} kubeadm-{{.KubeVersion}} kubectl-{{.KubeVersion}} --disableexcludes=kubernetes || sfFail 199
                            {{ else }}
                                    yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes || sfFail 199
                            {{ end }}
                                    ;;

                                *) echo "unsupported linux distribution ''"
                                   sfFail 200
                            esac
                            sfService enable kubelet && sfService start kubelet

                            sfExit

                    reverseproxy:
                        targets:
                            gateways: all
                        run: |
                            if [ -d ${SF_ETCDIR}/edgeproxy4network ]; then
                                TARGET_DIR=${SF_ETCDIR}/edgeproxy4network
                            elif [ -d ${SF_ETCDIR}/kong4gateway ]; then
                                TARGET_DIR=${SF_ETCDIR}/kong4gateway
                            fi
                            [ -z "${TARGET_DIR+x}" ] && sfFail 201

                            cat >${TARGET_DIR}/includes/kubernetes.conf <<-'EOF'
                            upstream kubernetes-api-cluster {
                                {{range .MasterIPs}}server {{.}}:6443;
                                {{end}}
                            }

                            server {
                                listen 6443;
                                proxy_pass kubernetes-api-cluster;
                            }
                            EOF
                            sfReverseProxyReload
                            sfExit

                    cp1-init:
                        targets:
                            masters: one
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            mkdir -p /etc/kubernetes/kubeadm

                            {{ if .KubeVersion }}
                            KUBE_VERSION=v{{ .KubeVersion }}
                            {{ else }}
                            KUBE_VERSION=stable
                            {{ end }}

                            cat >/etc/kubernetes/kubeadm/kubeadm-config.yaml <<-EOF
                            ---
                            apiVersion: kubeadm.k8s.io/v1beta1
                            kind: ClusterConfiguration
                            controlPlaneEndpoint: "{{.DefaultRouteIP}}:6443"
                            kubernetesVersion: ${KUBE_VERSION}
                            etcd:
                                local:
                                    dataDir: /var/lib/etcd
                            networking:
                                dnsDomain: cluster.local
                                podSubnet: 10.244.0.0/16
                                serviceSubnet: 10.96.0.0/12
                            {{ if eq .Hardening "true" }}
                            apiServer:
                                timeoutForControlPlane: 4m0s
                                extraArgs:
                                    #? API server is restarted because of health check fail
                                    #? Modern K8S deploys preferes RBAC admission associate to PodSecurityPolicy to authorization control
                                    # anonymous-auth: "false"
                                    ##? EventRateLimit error running kubeadm init, alpha version
                                    ##? DenyEscalatingExec, deprecated
                                    enable-admission-plugins: AlwaysPullImages,DefaultStorageClass,NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurityPolicy
                                    ## disable-admission-plugins: DynamicAuditing
                                    ##? error running kubeadm init
                                    # RBAC is used to grant permission to client, restrict access to anonymous users
                                    authorization-mode: RBAC,Node
                                    #!allow-privileged: "false"
                                    ##? kube-proxy cannot be run
                                    ##? RBAC admission provide fine grained control
                                    audit-log-path: /var/log/k8s_audit.log
                                    audit-log-maxage: "30"
                                    audit-log-maxbackup: "10"
                                    audit-log-maxsize: "100"
                                    service-account-lookup: "true"
                                    #? crash during init phase, to be investigated ...
                                    ## encryption-provider-config: aescbc
                                    profiling: "false"
                            controllerManager:
                                extraArgs:
                                    terminated-pod-gc-threshold: "12500"
                                    profiling: "false"
                                    use-service-account-credentials: "true"
                                    feature-gates: RotateKubeletServerCertificate=true
                            {{ end }}
                            certificatesDir: /etc/kubernetes/pki
                            clusterName: kubernetes
                            dns:
                                type: CoreDNS
                            imageRepository: k8s.gcr.io
                            EOF

                            sfRetry 5m 5 kubeadm config images pull || sfFail 202
                            kubeadm init --config=/etc/kubernetes/kubeadm/kubeadm-config.yaml || sfFail 203
                            cp_join_cmd=$(kubeadm token create --ttl 10m --print-join-command) || sfFail 204
                            cert_key=$(kubeadm init phase upload-certs --experimental-upload-certs | tail -n 1) || sfFail 205

                            cat >${SF_TMPDIR}/init_cluster_admin_kube.sh <<-'EOF'
                            mkdir -p ~{{.ClusterAdminUsername}}/.kube
                            cp -f /etc/kubernetes/admin.conf ~{{.ClusterAdminUsername}}/.kube/config
                            chown -R {{.ClusterAdminUsername}}:{{.ClusterAdminUsername}} ~{{.ClusterAdminUsername}}/.kube && \
                            chmod -R go-rwx ~{{.ClusterAdminUsername}}/.kube

                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/ca.crt
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/server.key
                            sudo chmod g+r /etc/kubernetes/pki/etcd/server.key
                            sudo chown root:{{.ClusterAdminUsername}} /etc/kubernetes/pki/etcd/server.crt
                            EOF

                            # execute init_cluster_admin_kube.sh on the current master
                            bash ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 206
                            # Starting from here, any kubectl command must be changed to sfKubectl

                            # Push control-plane join command to all the other masters
                            echo "$cp_join_cmd --experimental-control-plane --certificate-key $cert_key" >${SF_TMPDIR}/cp_join_cmd.sh
                            cat ${SF_TMPDIR}/cp_join_cmd.sh
                            sfDropzonePush ${SF_TMPDIR}/cp_join_cmd.sh || fail 207
                            sfDropzonePush ${SF_TMPDIR}/init_cluster_admin_kube.sh || fail 208
                            for ip in {{range .MasterIPs}}{{.}} {{end}}; do
                                [ "$ip" = "{{.HostIP}}" ] && continue
                                sfDropzoneSync $ip || sfFail 209
                            done
                            rm ${SF_TMPDIR}/cp_join_cmd.sh ${SF_TMPDIR}/init_cluster_admin_kube.sh
                            sfDropzoneClean

                            {{ if eq .Hardening "true" }}
                            cat >/etc/kubernetes/kubeadm/psp-default.yaml <<-'EOF'
                            apiVersion: extensions/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: default
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
                                seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
                            spec:
                              privileged: false
                              allowPrivilegeEscalation: false
                              allowedCapabilities: []  # default set of capabilities are implicitly allowed
                              volumes:
                              - 'configMap'
                              - 'emptyDir'
                              - 'projected'
                              - 'secret'
                              - 'downwardAPI'
                              - 'persistentVolumeClaim'
                              hostNetwork: false
                              hostIPC: false
                              hostPID: false
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            rules:
                            - apiGroups: ['policy']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['default']
                            - apiGroups: ['extensions']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['default']
                            ---
                            kind: ClusterRoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: default-psp
                            roleRef:
                              kind: ClusterRole
                              name: default-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                            - kind: Group
                              name: system:authenticated
                              apiGroup: rbac.authorization.k8s.io
                            EOF

                            cat >/etc/kubernetes/kubeadm/psp-privileged.yaml <<-'EOF'
                            apiVersion: extensions/v1beta1
                            kind: PodSecurityPolicy
                            metadata:
                              name: privileged
                              annotations:
                                seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
                            spec:
                              privileged: true
                              allowPrivilegeEscalation: true
                              allowedCapabilities: ['*']
                              volumes: ['*']
                              hostNetwork: true
                              hostPorts:
                              - min: 0
                                max: 65535
                              hostIPC: true
                              hostPID: true
                              runAsUser:
                                rule: 'RunAsAny'
                              seLinux:
                                rule: 'RunAsAny'
                              supplementalGroups:
                                rule: 'RunAsAny'
                              fsGroup:
                                rule: 'RunAsAny'
                            ---
                            kind: RoleBinding
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp-nodes
                              namespace: kube-system
                            roleRef:
                              kind: ClusterRole
                              name: privileged-psp
                              apiGroup: rbac.authorization.k8s.io
                            subjects:
                            # Allows access to resources required by the kubelet component
                            - kind: Group
                              apiGroup: rbac.authorization.k8s.io
                              name: system:nodes
                            # Allows access to resources required by the kubelet user
                            - kind: User
                              apiGroup: rbac.authorization.k8s.io
                              name: kubelet # Legacy node ID
                            # bind all service accounts in namespace kube-system (kube-proxy, CNI)
                            - apiGroup: rbac.authorization.k8s.io
                              kind: Group
                              name: system:serviceaccounts:kube-system
                            ---
                            kind: ClusterRole
                            apiVersion: rbac.authorization.k8s.io/v1
                            metadata:
                              name: privileged-psp
                            rules:
                            - apiGroups: ['policy']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['privileged']
                            - apiGroups: ['extensions']
                              resources: ['podsecuritypolicies']
                              verbs:     ['use']
                              resourceNames: ['privileged']
                            EOF

                            sfKubectl apply -f /etc/kubernetes/kubeadm/psp-default.yaml || sfFail 204
                            sfKubectl apply -f /etc/kubernetes/kubeadm/psp-privileged.yaml || sfFail 205
                            {{ end }}

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry 5m 10 sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 212
                            sfRetry 5m 10 sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 213
                            sfRetry 5m 10 sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 214

                            touch /etc/kubernetes/.joined
                            echo "cp1 init done"

                            sfExit

                    cni:
                        targets:
                            masters: one
                        run: |
                            case {{ .CNI }} in
                                flannel) # To be removed
                                    sfKubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml || sfFail 215

                                    sfFirewallAdd --zone=trusted --add-interface=flannel.1
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 216 "Firewall problem"
                                    ;;
                                calico)
                                    # Should already exist...
                                    mkdir -p /etc/kubernetes/kubeadm/

                                    # Applies RBAC Calico
                                    wget https://docs.projectcalico.org/v3.10/manifests/rbac/rbac-kdd-calico.yaml -P /etc/kubernetes/kubeadm/
                                    sfKubectl apply -f /etc/kubernetes/kubeadm/rbac-kdd-calico.yaml || sfFail 212

                                    # Downloads default calico
                                    wget https://docs.projectcalico.org/v3.10/manifests/calico.yaml -P /etc/kubernetes/kubeadm/

                                    # Creates our patch file
                                    cat >/etc/kubernetes/kubeadm/calico.yaml.patch <<-'EOF'
                            --- calico.yaml	2019-11-21 17:06:25.590230285 +0100
                            +++ calico.yaml.new	2019-11-21 17:02:11.885017690 +0100
                            @@ -24,7 +24,7 @@
                                "plugins": [
                                    {
                                    "type": "calico",
                            -          "log_level": "info",
                            +          "log_level": "debug",
                                    "datastore_type": "kubernetes",
                                    "nodename": "__KUBERNETES_NODE_NAME__",
                                    "mtu": __CNI_MTU__,
                            @@ -516,7 +516,7 @@
                                    # It can be deleted if this is a fresh installation, or if you have already
                                    # upgraded to use calico-ipam.
                                    - name: upgrade-ipam
                            -          image: calico/cni:v3.10.1
                            +          image: calico/cni:v3.10.0
                                    command: ["/opt/cni/bin/calico-ipam", "-upgrade"]
                                    env:
                                        - name: KUBERNETES_NODE_NAME
                            @@ -536,7 +536,7 @@
                                    # This container installs the CNI binaries
                                    # and CNI network config file on each node.
                                    - name: install-cni
                            -          image: calico/cni:v3.10.1
                            +          image: calico/cni:v3.10.0
                                    command: ["/install-cni.sh"]
                                    env:
                                        # Name of the CNI config file to create.
                            @@ -570,7 +570,7 @@
                                    # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
                                    # to communicate with Felix over the Policy Sync API.
                                    - name: flexvol-driver
                            -          image: calico/pod2daemon-flexvol:v3.10.1
                            +          image: calico/pod2daemon-flexvol:v3.10.0
                                    volumeMounts:
                                    - name: flexvol-driver-host
                                        mountPath: /host/driver
                            @@ -579,7 +579,7 @@
                                    # container programs network policy and routes on each
                                    # host.
                                    - name: calico-node
                            -          image: calico/node:v3.10.1
                            +          image: calico/node:v3.10.0
                                    env:
                                        # Use Kubernetes API as the backing datastore.
                                        - name: DATASTORE_TYPE
                            @@ -606,6 +606,8 @@
                                        value: "autodetect"
                                        # Enable IPIP
                                        - name: CALICO_IPV4POOL_IPIP
                            +              value: "Never"
                            +            - name: CALICO_IPV4POOL_VXLAN
                                        value: "Always"
                                        # Set MTU for tunnel device used if ipip is enabled
                                        - name: FELIX_IPINIPMTU
                            @@ -617,7 +619,7 @@
                                        # chosen from this range. Changing this value after installation will have
                                        # no effect. This should fall within `--cluster-cidr`.
                                        - name: CALICO_IPV4POOL_CIDR
                            -              value: "192.168.0.0/16"
                            +              value: "10.244.0.0/16"
                                        # Disable file logging so `kubectl logs` works.
                                        - name: CALICO_DISABLE_FILE_LOGGING
                                        value: "true"
                            @@ -753,7 +755,7 @@
                                priorityClassName: system-cluster-critical
                                containers:
                                    - name: calico-kube-controllers
                            -          image: calico/kube-controllers:v3.10.1
                            +          image: calico/kube-controllers:v3.10.0
                                    env:
                                        # Choose which controllers to run.
                                        - name: ENABLED_CONTROLLERS
                            EOF

                                    # Installs the 'patch' command
                                    case $(sfGetFact "linux_kind") in
                                        debian|ubuntu)
                                            sfApt update && sfApt install -y patch || sfFail 213
                                            ;;

                                        centos|redhat)
                                            yum -y install patch || sfFail 213
                                            ;;

                                        *) echo "unsupported linux distribution ''"
                                           sfFail 213
                                           ;;
                                    esac

                                    # Patches calico file
                                    patch -l /etc/kubernetes/kubeadm/calico.yaml /etc/kubernetes/kubeadm/calico.yaml.patch

                                    # Applies patched file
                                    sfKubectl apply -f /etc/kubernetes/kubeadm/calico.yaml || sfFail 214

                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 215 "Firewall problem"
                                    ;;
                            esac
                            sfExit 0

                    cpx-init:
                        targets:
                            masters: all
                        run: |
                            # Don't try to init a kubernetes cluster already running
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            sfDropzonePop ${SF_TMPDIR} || sfFail 223
                            sfDropzoneClean

                            [ -f ${SF_TMPDIR}/cp_join_cmd.sh ] || sfFail 217
                            bash ${SF_TMPDIR}/cp_join_cmd.sh || sfFail 218
                            [ -f ${SF_TMPDIR}/init_cluster_admin_kube.sh ] || sfFail 219
                            bash ${SF_TMPDIR}/init_cluster_admin_kube.sh || sfFail 220

                            # waits availability of key pods
                            set -u -o pipefail
                            sfRetry 5m 10 sfIsPodRunning kube-apiserver-{{.Hostname}}@kube-system || sfFail 228
                            sfRetry 5m 10 sfIsPodRunning kube-controller-manager-{{.Hostname}}@kube-system || sfFail 229
                            sfRetry 5m 10 sfIsPodRunning kube-scheduler-{{.Hostname}}@kube-system || sfFail 230

                            # Configure firewall
                            case {{.CNI}} in
                                flannel)
                                    sfFirewallAdd --zone=trusted --add-interface=flannel.1
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 231 "Firewall problem"
                                    ;;
                                calico)
                                    sfFirewallAdd --zone=trusted --add-interface=cni0
                                    sfFirewallReload || sfFail 225 "Firewall problem"
                                    ;;
                            esac

                            touch /etc/kubernetes/.joined
                            echo "cpx init done"
                            sfExit

                    join:
                        targets:
                            gateway: all
                            nodes: all
                        run: |
                            [ -f /etc/kubernetes/.joined ] && sfExit

                            MASTERIP=
                            for m in {{ range .MasterIPs }}{{.}} {{ end -}}; do
                                op=-1
                                NODE_JOIN_CMD=$(sfRemoteExec $m kubeadm token create --print-join-command) && op=$? || true
                                [ $op -ne 0 ] && continue
                                NODE_JOIN_CMD=$(echo $NODE_JOIN_CMD | head -1)
                                MASTERIP=$m
                                break
                            done
                            [ -z "$MASTERIP" ] && echo "failed to find available master to register with. Aborted." && sfFail 233
                            eval $NODE_JOIN_CMD || sfFail 234
                            touch /etc/kubernetes/.joined
                            sfExit

                    no-schedule:
                        targets:
                            gateways: all
                        run: |
                            {{-  if ne .AllowPodsOnGateways "true" }}
                            sfKubectl taint nodes {{ .Hostname }} key=value:NoSchedule || sfFail 235
                            {{ end }}
                            sfExit

                    helm-certs:
                        targets:
                            masters: one
                        run: |
                            mkdir -p ${SF_ETCDIR}/helm/pki ${SF_ETCDIR}/helm/ca
                            cd ${SF_TMPDIR}

                            ## Generate Certificate Authority
                            openssl genrsa -out ca.key.pem 4096
                            openssl req -key ca.key.pem -new -x509 \
                                -days 10000 -sha256 -out ca.cert.pem \
                                -extensions v3_ca -subj "/C=FR/ST=Toulouse/L=Toulouse/O=CSSI/OU=Space/CN={{ .Hostname }}"

                            ## Generate keys
                            openssl genrsa -out tiller.key.pem 4096
                            openssl genrsa -out helm.key.pem 4096

                            ## Create a config file for generating a Certificate Signing Request (CSR)
                            cat >csr.conf <<-EOF
                            [ req ]
                            default_bits = 2048
                            prompt = no
                            default_md = sha256
                            req_extensions = req_ext
                            distinguished_name = dn

                            [ dn ]
                            C=FR
                            ST=FR
                            L=Toulouse
                            O=CSSI
                            OU=Space
                            CN={{ .Hostname }}

                            [ req_ext ]
                            subjectAltName = @alt_names

                            [ alt_names ]
                            DNS.1 = helm
                            DNS.2 = helm.{{ .HelmDefaultNamespace }}
                            DNS.3 = helm.{{ .HelmDefaultNamespace }}.svc
                            DNS.4 = helm.{{ .HelmDefaultNamespace }}.svc.cluster
                            DNS.5 = helm.{{ .HelmDefaultNamespace }}.svc.cluster.local
                            DNS.6 = helm.{{ .HelmDefaultNamespace }}.svc.cluster.local.com
                            IP = {{ .HostIP }}

                            [ v3_ext ]
                            authorityKeyIdentifier=keyid,issuer:always
                            basicConstraints=CA:FALSE
                            keyUsage=keyEncipherment,dataEncipherment
                            extendedKeyUsage=serverAuth,clientAuth
                            subjectAltName=@alt_names
                            EOF

                            ## Generate the certificates signing request based on the config file
                            openssl req -new -key tiller.key.pem -out tiller.csr.pem -config csr.conf
                            openssl req -new -key helm.key.pem -out helm.csr.pem -config csr.conf

                            ## Generate certificates using the ca.key.pem, ca.crt.pem and xxxx.csr.pem
                            openssl x509 -req -CA ca.cert.pem -CAkey ca.key.pem -CAcreateserial -in tiller.csr.pem -out tiller.cert.pem -days 365 -extfile csr.conf

                            #openssl x509 -req -in helm.csr.pem -CA ca.cert.pem -CAkey ca.key.pem \
                            openssl x509 -req -CA ca.cert.pem -CAkey ca.key.pem -CAcreateserial -in helm.csr.pem -out helm.cert.pem  -days 365 -extfile csr.conf

                            ## Backup certificate and private key
                            mkdir -p ${SF_ETCDIR}/helm/pki
                            mv ca.cert.pem ca.key.pem ${SF_ETCDIR}/helm/ca
                            mv helm.cert.pem helm.key.pem ${SF_ETCDIR}/helm/pki
                            mv tiller.cert.pem tiller.key.pem ${SF_ETCDIR}/helm/pki

                            chown -R root:{{ .ClusterAdminUsername }} ${SF_ETCDIR}/helm
                            chmod -R u+rwx,g+r-wx,o-rwx ${SF_ETCDIR}/helm
                            find ${SF_ETCDIR}/helm -exec chmod ug+x {} \;

                            ## Clean directory
                            rm ca.srl csr.conf helm.csr.pem tiller.csr.pem
                            sfExit

                    helm-binary:
                        targets:
                            masters: all
                        run: |
                            # Install Helm
                            cd ${SF_TMPDIR}
                            wget https://get.helm.sh/helm-v{{.HelmVersion}}-linux-amd64.tar.gz || sfFail 236

                            tar -zxvf ${SF_TMPDIR}/helm-v{{.HelmVersion}}-linux-amd64.tar.gz -C ${SF_TMPDIR} || sfFail 237

                            mv ${SF_TMPDIR}/linux-amd64/helm /usr/local/bin && \
                            mv ${SF_TMPDIR}/linux-amd64/tiller /usr/local/bin && \
                            chmod a+rx /usr/local/bin/helm && \
                            chmod a+rx /usr/local/bin/tiller && \
                            rm -drf ${SF_TMPDIR}/linux-amd64 && \
                            rm -rf ${SF_TMPDIR}/helm-v{{.HelmVersion}}-linux-amd64.tar.gz || sfFail 238
                            sfExit

                    helm-account:
                        targets:
                            masters: one
                        run: |
                            ## Create serviceaccount
                            sfKubectl create serviceaccount tiller -n kube-system || sfFail 239
                            sfExit

                    helm-rbac-files:
                        targets:
                            masters: all
                        run: |
                            mkdir -p ${SF_ETCDIR}/helm

                            cat > ${SF_ETCDIR}/helm/tiller-clusterrolebinding-admin.yaml <<EOF
                            apiVersion: rbac.authorization.k8s.io/v1
                            kind: ClusterRoleBinding
                            metadata:
                                name: tiller-clusterrolebinding-admin
                            roleRef:
                                apiGroup: rbac.authorization.k8s.io
                                kind: ClusterRole
                                name: cluster-admin
                            subjects:
                                - kind: ServiceAccount
                                  name: tiller
                                  namespace: kube-system
                            EOF

                            sfExit

                    helm-rbac-apply:
                        targets:
                            masters: one
                        run: |
                            ## Apply RBAC policy
                            sfKubectl apply -f ${SF_ETCDIR}/helm/tiller-clusterrolebinding-admin.yaml || sfFail 240
                            sfExit

                    helm-init:
                        targets:
                            masters: one
                        run: |
                            ## Install Helm as {{ .ClusterAdminUsername }} (the user that runs kubectl) client with TLS authentication with Tiller
                            sudo -u {{ .ClusterAdminUsername }} -i helm init \
                                --tiller-tls \
                                --tiller-tls-hostname {{ .Hostname }} \
                                --tiller-tls-cert ${SF_ETCDIR}/helm/pki/tiller.cert.pem \
                                --tiller-tls-key ${SF_ETCDIR}/helm/pki/tiller.key.pem \
                                --tiller-tls-verify \
                                --tls-ca-cert ${SF_ETCDIR}/helm/ca/ca.cert.pem \
                                --service-account tiller \
                                --override 'spec.template.spec.containers[0].command'='{/tiller,--storage=secret}' \
                                || sfFail 247

                            mkdir -p /home/{{.ClusterAdminUsername}}/.helm
                            cp ${SF_ETCDIR}/helm/ca/ca.cert.pem /home/{{ .ClusterAdminUsername }}/.helm/ca.pem
                            cp ${SF_ETCDIR}/helm/pki/helm.cert.pem /home/{{ .ClusterAdminUsername }}/.helm/cert.pem
                            cp ${SF_ETCDIR}/helm/pki/helm.key.pem /home/{{ .ClusterAdminUsername }}/.helm/key.pem

                            chown -R {{ .ClusterAdminUsername }}:{{ .ClusterAdminUsername }} /home/{{ .ClusterAdminUsername }}/.helm
                            chmod -R 0770 /home/{{ .ClusterAdminUsername }}/.helm

                            # Push .helm configuration to all the other masters
                            # Creates an archive of the .helm directory
                            cd /home/{{ .ClusterAdminUsername }} && \
                            tar -zcvf ${SF_TMPDIR}/helm_config.tar.gz .helm || sfFail 248

                            # Push the archive in the dropzone and synchronize
                            sfDropzonePush ${SF_TMPDIR}/helm_config.tar.gz || sfFail 249
                            for ip in {{ range .MasterIPs }}{{ . }} {{ end }}; do
                                sfDropzoneSync $ip || sfFail 250
                            done
                            rm -f ${SF_TMPDIR}/helm_config.tar.gz
                            sfExit

                    helm-sync:
                        targets:
                            masters: all
                        run: |
                            sfDropzonePop ${SF_TMPDIR} || sfFail 251
                            sfDropzoneClean

                            [ -f ${SF_TMPDIR}/helm_config.tar.gz ] || sfFail 252
                            tar -zxvf ${SF_TMPDIR}/helm_config.tar.gz -C /home/{{ .ClusterAdminUsername }} || sfFail 253
                            rm -f ${SF_TMPDIR}/helm_config.tar.gz
                            sfExit

                    final:
                        targets:
                            masters: one
                        run: |
                            # Allows pods to start on master if there is only one master or if it's explicitely requested
                            if [ "{{.ClusterComplexity}}" = "small" -o "{{.AllowPodsOnMasters}}" = "true" ]; then
                                sfKubectl taint nodes --all node-role.kubernetes.io/master- || true
                            fi

                            # Adds namespace safescale
                            sfKubectl create namespace safescale
                            # FIXME: add label to namespace (in prevision of network policies)
                            # sfKubectl set label
                            # adds Kubernetes Dashboard
                            if [ "{{.Dashboard}}" == "true" ]; then
                                if curl --output /dev/null --silent --head --sfFail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                    sfRetry 3m 5 sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 254 "Unable to install kubernetes dashboard version $(sfGithubLastRelease)"
                                else
                                    if curl --output /dev/null --silent --head --sfFail "https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml"; then
                                        sfRetry 3m 5 sfKubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/$(sfGithubLastNotBetaRelease kubernetes dashboard)/src/deploy/recommended/kubernetes-dashboard.yaml || sfFail 255 "Unable to install kubernetes dashboard version $(sfGithubLastNotBetaRelease)"
                                    fi
                                fi
                            fi

                            sfExit

            remove:
                pace: helm-namespace,helm-cleanup,node,reset,reverseproxy,clean
                steps:
                    helm-namespace:
                        targets:
                            masters: one
                        run: |
                            ## Delete deployment
                            if sfKubectl -n kube-system get deployment tiller-deploy &>/dev/null; then
                                sfKubectl -n kube-system delete deployment tiller-deploy || sfFail 192
                            fi
                            ## Delete service
                            sfKubectl delete service -n kube-system tiller-deploy || sfFail 193
                            ## Delete account
                            if sfKubectl get serviceaccount tiller -n kube-system &>/dev/null; then
                                sfKubectl delete serviceaccount tiller -n kube-system || sfFail 194
                            fi
                            ## Delete secret
                            sfKubectl delete secret -n kube-system tiller-secret || sfFail 195
                            ## Delete RBAC policy
                            if sfKubectl get role tiller-rolebinding-secret -n kube-system &>/dev/null; then
                                sfKubectl delete -f ${SF_ETCDIR}/helm/tiller-clusterrolebinding-admin.yaml || sfFail 196
                            fi
                            sfExit

                    helm-cleanup:
                        targets:
                            masters: all
                        run: |
                            rm -f /usr/local/bin/helm || sfFail 201
                            rm -f /usr/local/bin/tiller || sfFail 202
                            rm -drf ${SF_ETCDIR}/helm /home/{{ .ClusterAdminUsername }}/.helm || sfFail 203
                            sfExit

                    node:
                        targets:
                            masters: one
                        run: |
                            sfKubectl drain {{.Hostname}} --delete-local-data --force --ignore-daemonsets
                            sfKubectl delete node {{.Hostname}}
                            sfExit

                    reverseproxy:
                        targets:
                            gateways: all
                        run: |
                            rm -f /etc/kong4gateway/includes/kubernetes.conf
                            sfReverseProxyReload
                            sfExit

                    reset:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |
                            kubeadm reset -f
                            sfExit

                    clean:
                        targets:
                            gateways: all
                            masters: all
                            nodes: all
                        run: |

                            case $LINUX_KIND in
                                debian|ubuntu)
                                    sfApt purge -y kubectl kubeadm kubelet || sfFail 204 "failure purging kubernetes"
                                    ;;
                                redhat|centos)
                                    yum remove -y kubectl kubeadm kubelet || sfFail 204 "failure purging kubernetes"
                                    ;;
                            esac
                            sfFirewallReload || sfFail 205 "Firewall problem"
                            rm -rf /etc/kubernetes/.joined /home/{{ .ClusterAdminUsername }}/.kube
                            sfExit

...
